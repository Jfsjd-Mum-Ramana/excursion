package org.vdsi.space.collections.customdatatransformer;

import java.util.concurrent.TimeUnit;

import org.apache.flink.api.common.JobID;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.restartstrategy.RestartStrategies;
import org.apache.flink.api.common.time.Time;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.configuration.CheckpointingOptions;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
import org.apache.flink.core.execution.JobClient;
import org.apache.flink.runtime.state.hashmap.HashMapStateBackend;
import org.apache.flink.runtime.state.storage.FileSystemCheckpointStorage;
import org.apache.flink.streaming.api.CheckpointingMode;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.CheckpointConfig;
import org.apache.flink.streaming.api.environment.CheckpointConfig.ExternalizedCheckpointCleanup;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.vdsi.space.collections.customdatatransformer.enums.JobStatus;
import org.vdsi.space.collections.customdatatransformer.util.AppProperties;
import org.vdsi.space.collections.customdatatransformer.util.PublishAuditMessage;
import org.vdsi.space.collections.lucene.CollectionAudit;

public class CsvTransformerJob {

	private static final Logger LOGGER = LoggerFactory.getLogger(CsvTransformerJob.class);
	public static AppProperties appProp = AppProperties.getInstance();
	String flinkJobName = "CSV to JSON Transformer Job";

	private CsvTransformerJobUtil jobUtil;
	private final PublishAuditMessage pubAuditMsg;

	public CsvTransformerJob(PublishAuditMessage pubAuditMsg) {
		this.jobUtil = new CsvTransformerJobUtil(pubAuditMsg);
		this.pubAuditMsg = pubAuditMsg;
	}

	public CsvTransformerJob(PublishAuditMessage pubAuditMsg,CsvTransformerJobUtil csvTransformerJobUtil) {
		this.jobUtil = csvTransformerJobUtil;
		this.pubAuditMsg = pubAuditMsg;
	}

	public PublishAuditMessage getPubAuditMsg() {
		return pubAuditMsg;
	}

	public void execute() throws Exception {

		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
		CollectionAudit collectionAudit = new CollectionAudit();

		try {

			env.setParallelism(Integer.valueOf(appProp.getAppProperties("flink.parallelism")));
	
			Configuration config = new Configuration();
			
			// Set up checkpointing
						env.enableCheckpointing(Long.valueOf(appProp.getAppProperties("flink.checkpoint.interval")));
						env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
						env.getCheckpointConfig().setMinPauseBetweenCheckpoints(Long.valueOf(appProp.getAppProperties("flink.checkpoint.min-pause")));
						env.getCheckpointConfig().setCheckpointTimeout(Long.valueOf(appProp.getAppProperties("flink.checkpoint.timeout")));
						env.getCheckpointConfig().setMaxConcurrentCheckpoints(Integer.valueOf(appProp.getAppProperties("flink.checkpoint.max-concurrent")));
						env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);
						// Set the state backend
						env.setStateBackend(new HashMapStateBackend());
						env.getCheckpointConfig().setCheckpointStorage(new FileSystemCheckpointStorage(appProp.getAppProperties("flink.checkpoint.checkpointdir")));
						env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, Time.of(10, TimeUnit.SECONDS)));



			OffsetsInitializer offsetsInitializer = jobUtil
					.getOffsetsInitializer(appProp.getAppProperties("spring.kafka.consumer.auto-offset-reset"));
			collectionAudit = jobUtil.setCollectionAuditProperties(collectionAudit);
			jobUtil.validateKafka(collectionAudit);

			KafkaSource<String> source = jobUtil.buildKafkaSource(collectionAudit, offsetsInitializer,
					appProp.getAppProperties("spring.kafka.consumer.group-id"));
			DataStream<String> stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), "Kafka Source");

			DataStream<CollectionAudit> collectionAuditStream = CsvTransformerJobUtil
					.convertStreamToCollectionAudit(stream, collectionAudit,pubAuditMsg);
			DataStream<Tuple2<String, CollectionAudit>> csvDataStream = CsvTransformerJobUtil
					.getCsvDataStream(collectionAuditStream,pubAuditMsg);
			DataStream<Tuple2<String, CollectionAudit>> jsonNodeStream = CsvTransformerJobUtil
					.convertCsvToJson(csvDataStream,pubAuditMsg);
			DataStream<Tuple2<String, CollectionAudit>> finalDataStream = CsvTransformerJobUtil
					.encapsulateJson(jsonNodeStream,pubAuditMsg);


			publishTransformedData(finalDataStream, null);

			JobClient jobClient = env.executeAsync(flinkJobName);
			JobID jobId = jobClient.getJobID();
			LOGGER.info("JobID is********************** {}", jobId);

		} catch (Exception e) {
			LOGGER.info("Error while executing CsvTransformerJob {}", e);
		}
	}

	public void publishTransformedData(DataStream<Tuple2<String, CollectionAudit>> encapsulatedJsonStream,
			String jobId) {
		try {
			encapsulatedJsonStream = jobUtil.filterNonEmptyRecords(encapsulatedJsonStream);
			
			DataStream<String> jsonStringStream = jobUtil.getJsonStringStream(encapsulatedJsonStream);

			if (jobUtil.shouldPublishKafka()) {
				jobUtil.publishToKafka(jsonStringStream);
			}

			if (jobUtil.shouldPublishVMB()) {
				jobUtil.publishToVMB(jsonStringStream);
			}

			jobUtil.processStream(encapsulatedJsonStream, JobStatus.FLINK_JOB_SUCCESSFUL,pubAuditMsg);

		} catch (Exception e) {
			LOGGER.error("Error while publishing data to Sink {} ", e);
			jobUtil.processStream(encapsulatedJsonStream, JobStatus.FLINK_JOB_FAILED,pubAuditMsg);
		}
	}
}

